/root/Targets/modcrawler
└─┬ modcrawler@0.1.2 
  ├─┬ jsdom@8.5.0 
  │ ├── abab@1.0.4 
  │ ├── acorn@2.7.0 
  │ ├── acorn-globals@1.0.9 
  │ ├── array-equal@1.0.0 
  │ ├── cssom@0.3.2 
  │ ├── cssstyle@0.2.37 
  │ ├─┬ escodegen@1.9.0 
  │ │ ├── esprima@3.1.3 
  │ │ ├── estraverse@4.2.0 
  │ │ ├── esutils@2.0.2 
  │ │ ├─┬ optionator@0.8.2 
  │ │ │ ├── deep-is@0.1.3 
  │ │ │ ├── fast-levenshtein@2.0.6 
  │ │ │ ├── levn@0.3.0 
  │ │ │ ├── prelude-ls@1.1.2 
  │ │ │ ├── type-check@0.3.2 
  │ │ │ └── wordwrap@1.0.0 
  │ │ └── source-map@0.5.7 
  │ ├── iconv-lite@0.4.19 
  │ ├── nwmatcher@1.4.1 
  │ ├── parse5@1.5.1 
  │ ├── sax@1.2.4 
  │ ├── symbol-tree@3.2.2 
  │ ├─┬ tough-cookie@2.3.3 
  │ │ └── punycode@1.4.1 
  │ ├── webidl-conversions@3.0.1 
  │ ├─┬ whatwg-url@2.0.1 
  │ │ └── tr46@0.0.3 
  │ └── xml-name-validator@2.0.1 
  ├── lodash@4.17.4 
  ├─┬ mkdirp@0.5.1 
  │ └── minimist@0.0.8 
  └─┬ request@2.82.0 
    ├── aws-sign2@0.7.0 
    ├── aws4@1.6.0 
    ├── caseless@0.12.0 
    ├─┬ combined-stream@1.0.5 
    │ └── delayed-stream@1.0.0 
    ├── extend@3.0.1 
    ├── forever-agent@0.6.1 
    ├─┬ form-data@2.3.1 
    │ └── asynckit@0.4.0 
    ├─┬ har-validator@5.0.3 
    │ ├─┬ ajv@5.2.2 
    │ │ ├── co@4.6.0 
    │ │ ├── fast-deep-equal@1.0.0 
    │ │ ├── json-schema-traverse@0.3.1 
    │ │ └─┬ json-stable-stringify@1.0.1 
    │ │   └── jsonify@0.0.0 
    │ └── har-schema@2.0.0 
    ├─┬ hawk@6.0.2 
    │ ├── boom@4.3.1 
    │ ├─┬ cryptiles@3.1.2 
    │ │ └── boom@5.2.0 
    │ ├── hoek@4.2.0 
    │ └── sntp@2.0.2 
    ├─┬ http-signature@1.2.0 
    │ ├── assert-plus@1.0.0 
    │ ├─┬ jsprim@1.4.1 
    │ │ ├── extsprintf@1.3.0 
    │ │ ├── json-schema@0.2.3 
    │ │ └─┬ verror@1.10.0 
    │ │   └── core-util-is@1.0.2 
    │ └─┬ sshpk@1.13.1 
    │   ├── asn1@0.2.3 
    │   ├── bcrypt-pbkdf@1.0.1 
    │   ├── dashdash@1.14.1 
    │   ├── ecc-jsbn@0.1.1 
    │   ├── getpass@0.1.7 
    │   ├── jsbn@0.1.1 
    │   └── tweetnacl@0.14.5 
    ├── is-typedarray@1.0.0 
    ├── isstream@0.1.2 
    ├── json-stringify-safe@5.0.1 
    ├─┬ mime-types@2.1.17 
    │ └── mime-db@1.30.0 
    ├── oauth-sign@0.8.2 
    ├── performance-now@2.1.0 
    ├── qs@6.5.1 
    ├── safe-buffer@5.1.1 
    ├── stringstream@0.0.5 
    ├── tunnel-agent@0.6.0 
    └── uuid@3.1.0 

Setup Done exists, not setting up
:../FeatureTester/libs/:/root/Targets/modcrawler/node_modules
Set Default Z3_PATH to ./node_modules/z3javascript/bin/libz3.so
ExpoSE Master: /root/ExpoSE/lib/Harness/src/harness.js max concurrent: 16 max paths: 1000000
Setting timeout to 900000
*** [0 done /0 queued / 1 running / 0 errors / 0% coverage ] ****** [0 done /0 queued / 1 running / 0 errors / 0% coverage ] ****** [1 done /0 queued / 0 running / 1 errors / 33% coverage ] ***
*-- Stat Module Output --*
*-- concretizations: ["defineProperty","bound log"]
*-- Stat Module Done --*
*-- Test Case {"_bound":0} start 0.0963 took 7.7733s
*-- Errors occured in test {"_bound":0}
* Error: Tropigate failed because SyntaxError: Unexpected character '#' (1:78) on program #! /usr/bin/env node
var request   = require('request');
var fs        = require('fs');
var mkdirp    = require('mkdirp');
var jsdom     = require('jsdom');
var urlParser = require('url');
var lodash    = require('lodash');

var defaultModule =  {
	////-----------------------------------------------------------------------------------------
	// crawls the link and searches for others, and saves those to the cache
	crawler: {
		////-----------------------------------------------------------------------------------------
		// the index of the queue
		current: 0,
		////-----------------------------------------------------------------------------------------
		// list of url-strings
		queue:   [],
		////-----------------------------------------------------------------------------------------
		// says what type of url it is, but this is optional for some cases
		context: [],
		////-----------------------------------------------------------------------------------------
		// whats the first index to use
		startContext: 'index',
		////-----------------------------------------------------------------------------------------
		// custom headers
		headers: {
		},
		load: function() {
			while(this.current < this.queue.length) {
				this.request();
			}
		},
		////-----------------------------------------------------------------------------------------
		// Handles requests, or takes the cached version
		request: function() {
			var url     = urlParser.parse(this.queue[this.current]);
			var cb      = serious.scraper.response.bind(serious.scraper, this.current, url);
			var cache = serious.cache.get(url);
			if(cache === false) {
				var opt = {
					url: url.href,
					headers: this.headers
				};
				request(opt, function (error, response, body) {
					console.log('Loading: ' + url.href);
					if (!error && response.statusCode == 200) {
						// added index to scraper call
						serious.cache.set(url, body);
						jsdom.env(body, serious.scraper.includes, cb);
					} else {
						if(!response) response = {};
						console.error(url.href + ' Failed', error, response.statusCode, body);
					}
				});
			} else {
				jsdom.env(cache, serious.scraper.includes, cb);
			}
			this.current++;
		},
		////-----------------------------------------------------------------------------------------
		// Checks if a url is already processed
		addRequest: function(url, context) {
			if(!this.isLoaded(url)) {
				// @TODO add host check
				this.queue.push(url);
				this.context.push(context);
			} else {
				console.warn('The url was already in the system ' + url);
			}
		},
		////-----------------------------------------------------------------------------------------
		// Checks if url already got processed, does not check filesystem cache
		isLoaded: function(url) {
			if( this.queue.indexOf(url) === -1 ) {
				return false;
			} else {
				return true;
			}
		},
	},
	////-----------------------------------------------------------------------------------------
	// parses data from the body
	scraper: {
		////-----------------------------------------------------------------------------------------
		// external libraries like jquery can be added here
		includes: [],
		////-----------------------------------------------------------------------------------------
		// the place where your scraper module should put data in
		data: {},
		////-----------------------------------------------------------------------------------------
		// prefix where scraper data should be saved
		prefix: 'dist/scraper',
		////-----------------------------------------------------------------------------------------
		// main function for scraping the relevant informations
		response: function(index, url, err, window) {
			console.log(index, window.document.links.length, window.document.getElementsByTagName('li').length);

		},
		////-----------------------------------------------------------------------------------------
		// When everything is crawled, this function gets triggered
		done: function() {
			console.log(this.data);
		},
		////-----------------------------------------------------------------------------------------
		// saves scraper data
		save: function(opt, data) {
			serious.cache.set(opt, data, this.prefix);
		}
	},
	module: {
		////-----------------------------------------------------------------------------------------
		// overwrites this function by your module
		init: function(parent) {
			throw 'Overwrite this function to get the scope of the parent';
		},
		////-----------------------------------------------------------------------------------------
		// overwrites the module, depending on the host
		load: function(host) {
			try {
				var mod = require('./modules/' + host);
				lodash.merge(serious, mod);
				serious.module.init(serious);
			} catch(err) {
				console.warn('Could not load a module for ' + host, err);
			}
		}
	},
	////-----------------------------------------------------------------------------------------
	// handles the caching of the requests
	cache: {
		////-----------------------------------------------------------------------------------------
		// delimiter
		delimiter: '/',
		////-----------------------------------------------------------------------------------------
		// where should the cached be saved in
		prefix: 'dist/cache/crawler',
		////-----------------------------------------------------------------------------------------
		// checks if a request got cached, then it returns the body, else it returns false
		get: function(url) {
			try {
				var path = this.prefix + this.delimiter + url.host + this.delimiter + url.path;
				var file = fs.readFileSync(path, 'utf8');
				return file;
			} catch(err){
				return false;
			}
		},
		////-----------------------------------------------------------------------------------------
		// saves a body to the cache
		set: function(url, body, prefix) {
			if(!prefix) prefix = this.prefix;
			var delimiter = this.delimiter;
			var pathParts = url.path.split(delimiter);
			var fileName  = pathParts.pop();
			var path = prefix + delimiter + url.host + delimiter + pathParts.join(delimiter);
			mkdirp(path, function(err) {
				fs.writeFileSync(path + delimiter + fileName, body);
			});
		}
	},
	////-----------------------------------------------------------------------------------------
	// helper function for cli interface
	process: {
		////-----------------------------------------------------------------------------------------
		// parses the command line interface
		parse: function(args) {
			var result = null;
			var key = null;
			var opt = {};
			for(var i = 2; i < process.argv.length; i++) {
				var para = process.argv[i];
				if(para.indexOf('--') === 0) {
					if(key) throw key + ' had no corresponding value';
					key = para.slice(2, para.length);
				} else if(key) {
					opt[key] = para; // @TODO implement --header.cookie
					key = null;
				} else {
					if(result) throw 'There was already an url set ' + url;
					result = urlParser.parse(para, true);
				}
			}

			if(key) {
				throw 'There was no value set for ' + key;
			}
			return {url: result, opt: opt};
		}
	}
};

var serious = {
	crawler: {
		////-----------------------------------------------------------------------------------------
		// Sets up the initial request and overwrites functions from the module depending of host
		init: function(opt) {
			lodash.merge(serious, defaultModule);
			this.host = opt.url.host;
			serious.module.load(opt.url.host);
			this.addRequest(opt.url.href, this.startContext);
			this.load();
		}
	}
};

if(module.parent === null) { // Called via cli
	var url = defaultModule.process.parse(process.argv, true);

	if(url) {
		serious.crawler.init(url);
	}
} else { // Called via require
	// currently is not useful, but maybe later on
	lodash.merge(serious, defaultModule);
	module.exports = serious;
}
 at SyntaxError: Unexpected character '#' (1:78)
    at Parser.pp$4.raise (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:2221:15)
    at Parser.pp$7.getTokenFromCode (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:2756:10)
    at Parser.pp$7.readToken (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:2477:17)
    at Parser.readToken (/root/ExpoSE/lib/Tropigate/bin/Tokens.js:124:26)
    at Parser.pp$7.nextToken (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:2468:15)
    at Parser.pp$7.next (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:2413:10)
    at Parser.pp.eat (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:536:12)
    at Parser.pp.semicolon (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:581:15)
    at Parser.pp$1.parseVarStatement (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:918:10)
    at Parser.pp$1.parseStatement (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:706:19)
*-- Replay with NO_COMPILE=1 expoSE replay '/root/ExpoSE/lib/Harness/src/harness.js' '{"_bound":0}'
*-- Coverage Data
*- File /root/ExpoSE/lib/Harness/src/harness.js. Coverage (Term): 18% Coverage (LOC): 24%
*- File /root/ExpoSE/lib/S$/bin/symbols.js. Coverage (Term): 16% Coverage (LOC): 34%
*- Re-run with EXPOSE_PRINT_COVERAGE=1 to print line by line coverage information
** ExpoSE Finished. 1 paths with 1 errors **

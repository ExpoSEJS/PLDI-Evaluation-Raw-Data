/root/Targets/confluence2markdown
└─┬ confluence2markdown@0.0.5 
  └─┬ tokenizer@1.1.2-pbc  (git://github.com/pborenstein/node-tokenizer.git#d366ab3f3f2b816e71f18e07dc6cefccfa116738)
    └── disect@1.1.1 

Setup Done exists, not setting up
:../FeatureTester/libs/:/root/Targets/confluence2markdown/node_modules
Set Default Z3_PATH to ./node_modules/z3javascript/bin/libz3.so
ExpoSE Master: /root/ExpoSE/lib/Harness/src/harness.js max concurrent: 16 max paths: 1000000
Setting timeout to 900000
*** [0 done /0 queued / 1 running / 0 errors / 0% coverage ] ****** [0 done /0 queued / 1 running / 0 errors / 0% coverage ] ****** [1 done /0 queued / 0 running / 1 errors / 21% coverage ] ***
*-- Stat Module Output --*
*-- concretizations: ["defineProperty","bound log"]
*-- Stat Module Done --*
*-- Test Case {"_bound":0} start 0.0208 took 3.0359s
*-- Errors occured in test {"_bound":0}
* Error: Tropigate failed because SyntaxError: Unexpected token (3:4) on program var EventEmitter = require('events').EventEmitter;
var util = require('util');
var assert = require('assert');
var Transform = require('stream').Transform;
var disect = require('disect');

function noop(){}

function Tokenizer (check_token_cb, options) {
    if(!(this instanceof Tokenizer)) {
      return new Tokenizer(check_token_cb);
    }

    Transform.call(this, options);
    this._readableState.objectMode = true;
    this._buffered = ""; // we buffer untokenized data between writes
    this._regexes = []; // should contain objects
                        // with regex[RegExp] and type[String]
    this._ignored = {}; // a hash of ignored token types
                        // these will be parsed but not emitted
    this._checkToken = check_token_cb || noop;
}
util.inherits(Tokenizer, Transform);

Tokenizer.prototype._transform = function _transform(chunk, encoding, callback) {
  chunk = chunk.toString();
  var self = this;
  process.nextTick(function () {
    try {
      var index = 0, step = 64;
      while(index < chunk.length) {
        self._tokenize(chunk.substr(index, step));
        index += step;
      }
      callback();
    } catch(e) {
      callback(e);
    }
  })
};

Tokenizer.prototype._getMatchingRule = function _getMatchingRule(str) {
  for (var i = 0; i < this._regexes.length; ++i) {
      if(this._regexes[i].regex.test(str)) {
        return this._regexes[i];
      }
  }
  return null;
};

Tokenizer.prototype._tokenize = function _tokenize(data, nobuffer) {
    var r = -1;
    var regexes = this._regexes;
    // in case we buffered data on previous writes
    data = this._buffered + data;
    this._buffered = '';
    if(!data.length) {
      return;
    }

    var self = this;
    var maxIndex = disect(0, data.length, function (index) {
      var buf = data.substring(0, index + 1);
      return self._getMatchingRule(buf) === null;
    });

    if(maxIndex === 0) {
      // disect is fine if the RE that describes
      //  the token also describes a subset of the token
      //  But maybe there's a trailing literal?
      //  Then disect's strategy of trying shorter and shorter
      //  substrings if there isn't a match will fail.
      //  So let's take a brute force approach and keep
      //  going forward until we find a matching token.
      //  Of course, we could end up going to the end
      //  of what might be a very large text.

      for (i = 0, r = -1; i < data.length+1; i++) {
        if (self._getMatchingRule(data.substring(0,i))) {
          r = i;
          break;
        }
      }
      if (r > 0) {
        maxIndex = r;
      }

      if (r === 0) {
        throw new Error('r is 0?');
      }

      if (r < 0) {
        // We've tried matching what we have so far
        // but maybe we just haven't seen the end
        // of the token yet.

         maxIndex = data.length;
      }

    }

        // no token should be more that 128 chars
        //  of course someone will have a legitimate reason.
        //  turns out link tokens can be pretty long
    if(maxIndex === 0 || maxIndex > 1024) {
      // no match found
      throw new SyntaxError('could not tokenize ' + JSON.stringify(data));
    }
    else if (maxIndex === data.length && !nobuffer) {
      // the whole string is matching
      this._buffered = data;
      return;
    }
    else {
      // some substring is matching
      var str = data.substring(0, maxIndex);
      var rule = this._getMatchingRule(str);
      if(!rule) {
        throw new Error('wutt? ' + JSON.stringify(data));
      }
      this._gotToken(str, rule);
      this._tokenize(data.substring(maxIndex), nobuffer);
    }
};

Tokenizer.prototype._flush = function _flush(callback) {
  var self = this;
  process.nextTick(function () {
    try {
      self._tokenize('', true);
      callback();
    } catch(e) {
      callback(e);
    }
  });
};

var Token = function String (content, type) {
  this.content = content;
  this.type = type;
  this.toString = function () {
    return this.content.toString();
  }
}
util.inherits(Token, String);
Token.prototype.valueOf = function valueOf() {
  return this.content;
};

Tokenizer.prototype._gotToken = function _gotToken(str, rule) {
    // notify the token checker
    var type = this._checkToken(str, rule) || rule.type;
    if(this._ignored[type]) return;
    var token = new Token(str, type);

    this.push(token);

    this.emit('token', token, type);
};

Tokenizer.prototype.addRule = function addRule(regex, type) {
    // this is useful for built-in rules
    if(!type) {
      if(Array.isArray(regex)) {
        return this.addRule(regex[0], regex[1]);
      }
      else if(regex) {
        return this.addRule(Tokenizer[regex]);
      }
      else {
        throw new Error('No parameters specified');
      }
    }
    assert.ok((regex instanceof RegExp) || (typeof regex === 'function'));
    assert.equal(typeof type, 'string');
    this._regexes.push({regex:regex,type:type});
};

/**
 * set some tokens to be ignored. these won't be emitted
 */
Tokenizer.prototype.ignore = function ignore(ignored) {
    if(Array.isArray(ignored)) {
        for (var i = 0; i < ignored.length; ++i) {
            this.ignore(ignored[i]);
        }
        return;
    }
    this._ignored[ignored] = true;
};

module.exports = Tokenizer;

// built-in rules
Tokenizer.whitespace    = [/^(\s)+$/, 'whitespace'];
Tokenizer.word          = [/^\w+$/, 'word'];
Tokenizer.number        = [/^\d+(\.\d+)?$/, 'number'];
 at SyntaxError: Unexpected token (3:4)
    at Parser.pp$4.raise (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:2221:15)
    at Parser.pp.unexpected (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:603:10)
    at Parser.pp$2.parseBindingAtom (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:1417:12)
    at Parser.parseBindingAtom (/root/ExpoSE/lib/Tropigate/bin/FunctionSignatures.js:49:30)
    at Parser.pp$1.parseVarId (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:1049:20)
    at Parser.pp$1.parseVar (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:1032:14)
    at Parser.pp$1.parseVarStatement (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:917:10)
    at Parser.pp$1.parseStatement (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:706:19)
    at Parser.parseStatement (/root/ExpoSE/lib/Tropigate/bin/Statements.js:104:30)
    at Parser.pp$1.parseTopLevel (/root/ExpoSE/lib/Tropigate/node_modules/acorn/dist/acorn.js:638:25)
*-- Replay with NO_COMPILE=1 expoSE replay '/root/ExpoSE/lib/Harness/src/harness.js' '{"_bound":0}'
*-- Coverage Data
*- File /root/ExpoSE/lib/Harness/src/harness.js. Coverage (Term): 18% Coverage (LOC): 24%
*- File /root/ExpoSE/lib/S$/bin/symbols.js. Coverage (Term): 16% Coverage (LOC): 34%
*- File /root/Targets/confluence2markdown/node_modules/confluence2markdown/lib/Confluence2Markdown.js. Coverage (Term): 6% Coverage (LOC): 8%
*- File /root/Targets/confluence2markdown/node_modules/confluence2markdown/lib/ConfluenceTokenizer.js. Coverage (Term): 5% Coverage (LOC): 8%
*- Re-run with EXPOSE_PRINT_COVERAGE=1 to print line by line coverage information
** ExpoSE Finished. 1 paths with 1 errors **
